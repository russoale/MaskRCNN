{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Root directory to hdd\n",
    "DATA_DIR = os.path.abspath(\"/data/hdd\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import data_generator\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "from mrcnn.load_weights import load_weights\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(DATA_DIR, \"russales\", \"test_logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "JUMP_MODEL_PATH = os.path.join(DATA_DIR, \"russales\", \"logs\", \"jump20190413T1600\", \"mask_rcnn_jump_0160.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "GPU_ID = \"0\"\n",
    "DEVICE = \"/gpu:{}\".format(GPU_ID)  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\"\n",
    "LIMIT = 0\n",
    "EVAL_TYPE = \"keypoints\"\n",
    "HEADS = \"keypoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        25\n",
      "DETECTION_MIN_CONFIDENCE       0\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "KEYPOINT_LOSS_WEIGHTING        True\n",
      "KEYPOINT_MASK_POOL_SIZE        14\n",
      "KEYPOINT_MASK_SHAPE            [56, 56]\n",
      "KEYPOINT_THRESHOLD             0.005\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'mrcnn_mask_loss': 1.0, 'rpn_bbox_loss': 1.0, 'rpn_class_loss': 1.0, 'mrcnn_keypoint_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               128\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           jump\n",
      "NUM_CLASSES                    2\n",
      "NUM_KEYPOINTS                  20\n",
      "PART_STR                       ['head', 'neck', 'r_shoulder', 'r_elbow', 'r_wrist', 'r_hand', 'l_shoulder', 'l_elbow', 'l_wrist', 'l_hand', 'r_hip', 'r_knee', 'r_ankle', 'r_heel', 'r_toetip', 'l_hip', 'l_knee', 'l_ankle', 'l_heel', 'l_toetip']\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    150\n",
      "STEPS_PER_EPOCH                704\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAINING_HEADS                 keypoint\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           30\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               49\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jump\n",
    "class InferenceConfig(jump.JumpConfig):\n",
    "\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0\n",
    "    TRAINING_HEADS = HEADS\n",
    "\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Skeleton: (0,)\n",
      "Keypoint names: (20,)\n",
      "Test Keypoints Image Count: 918\n",
      "Test Keypoints Class Count: 2\n",
      "  0. BG                                                \n",
      "  1. person                                            \n"
     ]
    }
   ],
   "source": [
    "# Test dataset\n",
    "JUMP_DIR = os.path.join(DATA_DIR, \"russales\", \"JumpDataset\", \"mscoco_format\")\n",
    "dataset_test = jump.JumpDataset()\n",
    "jump_dataset = dataset_test.load_jump(JUMP_DIR, \"test\", return_jump=True)\n",
    "dataset_test.prepare()\n",
    "\n",
    "print(\"Test Keypoints Image Count: {}\".format(len(dataset_test.image_ids)))\n",
    "print(\"Test Keypoints Class Count: {}\".format(dataset_test.num_classes))\n",
    "for i, info in enumerate(dataset_test.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  /data/hdd/russales/logs/jump20190413T1600/mask_rcnn_jump_0160.h5\n",
      "Re-starting from epoch 160\n"
     ]
    }
   ],
   "source": [
    "# Create model in inference mode\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_ID\n",
    "\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=TEST_MODE, config=config, model_dir=MODEL_DIR)\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading weights \", JUMP_MODEL_PATH)\n",
    "load_weights(model, JUMP_MODEL_PATH, by_name=True, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_jump_results(dataset, image_ids, result):\n",
    "    # Arrange resutls to match COCO specs in http://cocodataset.org/#format\n",
    "    # If no results, return an empty list\n",
    "\n",
    "    if result[\"bboxes\"] is None:\n",
    "        return []\n",
    "\n",
    "    rois = result[\"bboxes\"]\n",
    "    class_ids = result[\"class_ids\"]\n",
    "    scores = result[\"scores\"]\n",
    "    masks = None\n",
    "    keypoints = None\n",
    "\n",
    "    if \"masks\" in result:\n",
    "        masks = result[\"masks\"].astype(np.uint8)\n",
    "\n",
    "    if \"keypoints\" in result:\n",
    "        keypoints = result[\"keypoints\"]\n",
    "\n",
    "    results = []\n",
    "    for image_id in image_ids:\n",
    "        # Loop through detections\n",
    "        for i in range(rois.shape[0]):\n",
    "            class_id = class_ids[i]\n",
    "            score = scores[i]\n",
    "            bbox = np.around(rois[i], 1)\n",
    "\n",
    "            result = {\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": dataset.get_source_class_id(class_id, \"jump\"),\n",
    "                \"bbox\": [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]],\n",
    "                \"score\": score\n",
    "            }\n",
    "\n",
    "            if masks is not None:\n",
    "                mask = masks[:, :, i]\n",
    "                result[\"segmentation\"] = maskUtils.encode(np.asfortranarray(mask))\n",
    "            if keypoints is not None:\n",
    "                keypoint = keypoints[i, :, :].flatten().tolist()\n",
    "                result[\"keypoints\"] = keypoint\n",
    "\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_jump(model, dataset, jump, eval_type=\"bbox\", limit=0, image_ids=None, training_heads=None):\n",
    "    \"\"\"Runs a jump evaluation.\n",
    "\n",
    "        model: MaskRCNN model for inference\n",
    "        dataset: A Dataset object with valiadtion data\n",
    "        jump: JUMP class to load annotations and result\n",
    "        eval_type: 'segm', 'bbox' (official COCO evaluation) or 'keypoints' for jump evaluation\n",
    "        limit: if not 0, it's the number of images to use for evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # Pick image_ids from the dataset\n",
    "    image_ids = image_ids or dataset.image_ids\n",
    "\n",
    "    if eval_type == 'segm':\n",
    "        # filter annotations for existing masks\n",
    "        valid_ids = []\n",
    "        for id in image_ids:\n",
    "            _, _, mask_train = dataset.load_mask(id)\n",
    "            if mask_train == 1:\n",
    "                valid_ids.append(id)\n",
    "        image_ids = valid_ids\n",
    "\n",
    "    # Limit to a subset\n",
    "    if limit and limit < len(image_ids):\n",
    "        image_ids = image_ids[:limit]\n",
    "\n",
    "    # Get corresponding COCO image IDs.\n",
    "    jump_image_ids = [dataset.image_info[id][\"id\"] for id in image_ids]\n",
    "\n",
    "    t_prediction = 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    results = []\n",
    "    total_image_count = len(image_ids)\n",
    "    next_prgress = 0\n",
    "    for i, image_id in enumerate(image_ids):\n",
    "        # print progress\n",
    "        progress = int(100 * (i / total_image_count))\n",
    "        if next_prgress != progress:\n",
    "            next_prgress = progress\n",
    "            print('\\r[{0}{1}] {2}%'.format('#' * progress, \" \" * (100 - progress), progress), end=' ', flush=True)\n",
    "\n",
    "        # Load image\n",
    "        image = dataset.load_image(image_id)\n",
    "\n",
    "        # Run detection\n",
    "        t = time.time()\n",
    "        if training_heads == \"keypoint\":\n",
    "            r = model.detect_keypoint([image], verbose=0)[0]\n",
    "        elif training_heads == \"mask\":\n",
    "            r = model.detect_mask([image], verbose=0)[0]\n",
    "        else:\n",
    "            r = model.detect([image], verbose=0)[0]\n",
    "\n",
    "        t_prediction += (time.time() - t)\n",
    "\n",
    "        # Convert results to COCO format\n",
    "        # Cast masks to uint8 because COCO tools errors out on bool\n",
    "        image_results = build_jump_results(dataset, jump_image_ids[i:i + 1], r)\n",
    "        results.extend(image_results)\n",
    "\n",
    "    print(\"\")\n",
    "    if training_heads == \"mask\":\n",
    "\n",
    "        results = [i for i in results if i['segmentation']['counts'] != bytes(b'PPYo1')]\n",
    "        if results:\n",
    "            # Load results. This modifies results with additional attributes.\n",
    "            jump_results = jump.loadRes(results)\n",
    "\n",
    "            # Evaluate\n",
    "            from pycocotools.cocoeval import COCOeval\n",
    "            cocoEval = COCOeval(cocoGt=jump, cocoDt=jump_results, iouType=eval_type)\n",
    "            cocoEval.params.imgIds = jump_image_ids\n",
    "            cocoEval.evaluate()\n",
    "            cocoEval.accumulate()\n",
    "            cocoEval.summarize()\n",
    "\n",
    "    if training_heads == \"keypoint\":\n",
    "        import samples.jump.pose_metrics\n",
    "        from samples.jump.bisp_joint_order import JumpJointOrder\n",
    "\n",
    "        predictions = []\n",
    "        annotations = []\n",
    "        for result in results:\n",
    "            predictions.append(samples.jump.pose_metrics.np.array(result[\"keypoints\"]).reshape((1, 20, 3)))\n",
    "            annotations.append(samples.jump.pose_metrics.np.array(\n",
    "                [i for i in dataset.image_info if i['id'] == result['image_id']][0][\"annotations\"][0][\n",
    "                    \"keypoints\"]).reshape((1, 20, 3)))\n",
    "\n",
    "        norm_distance = samples.jump.pose_metrics.pck_normalized_distances_fast(np.asarray(predictions).squeeze(),\n",
    "                                                                                np.asarray(annotations).squeeze(),\n",
    "                                                                                ref_length_indices=(\n",
    "                                                                                    JumpJointOrder.l_shoulder,\n",
    "                                                                                    JumpJointOrder.r_hip))\n",
    "        # filter norm_distance for invalid flags\n",
    "        norm_distance = np.where(norm_distance > 0, norm_distance, -1)\n",
    "\n",
    "        # plot\n",
    "        pck_thresholds, pck_scores = samples.jump.pose_metrics.pck_scores_from_normalized_distances(norm_distance)\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(pck_thresholds, pck_scores)\n",
    "        plt.xlabel('PCK thresholds')\n",
    "        plt.ylabel('PCK scores')\n",
    "        plt.axis([0, pck_thresholds.max(), 0, pck_scores.max()])\n",
    "        plt.show()\n",
    "\n",
    "        score = samples.jump.pose_metrics.pck_score_at_threshold(pck_thresholds, pck_scores, 0.2)\n",
    "        print(\"Avg. PCK {}\".format(score))\n",
    "\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Prediction time: {}. Average {}/image\".format(t_prediction, t_prediction / len(image_ids)))\n",
    "    print(\"Total time: \", time.time() - t_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[################################################################################################### ] 99% \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAG45JREFUeJzt3XuUXXV99/H3Zy65kgRCwi2JJGIgDSHcIlqpig8UA7iIPiKC6BKrpBew2iottj6oPPZicdn2WaVKEB6UKgg81aYaTZFLoQqacA1JuIRwSWJCLkAScp3L9/lj7xlOTmbO2aHnlzN7+LzWmjVn77PPPt+ZNbM/5/f77f3bigjMzMwAWppdgJmZDRwOBTMz6+VQMDOzXg4FMzPr5VAwM7NeDgUzM+uVLBQk3SBpvaTH+3lekv6PpBWSHpN0UqpazMysmJQthRuB2TWePwuYmn/NBb6ZsBYzMysgWShExL3ASzU2mQN8NzIPAAdKOjxVPWZmVl9bE997ArCqYnl1vm5t9YaS5pK1Jhg5cuTJ06ZN2y8FDlQBdEfQ3Q0RkT2OoDuy9VH1vbv3e76uu3Lb7HFU7hyI19Zkj2KPpyH62KZ3fWWtsfdrzWy/271uxcaIGF9vu2aGQmERMQ+YBzBr1qxYvHhxkyvad93dwSs7Onhxy05e2d7Blp0dbNnRweYdHazdvBOAHR1d7NjdxfbdnWzf3cXOji627+5Zl63f2dHN7q7ufXrvFmBoqxjW3srw9laGD8m+9ywPbW+hVUISLYIWiZYWkITIl/P12uMxe76mdzl7LtVrBEh7/ozZ2t6FqueqlqtevPfz/S+rauvqbavty3vV23e9Oqu3UP+/kv/W7yDbPuHvu2KLer+Dvd+7+O9wr7pex++7Z50q9lf596mKbehnvfRa3dnjym37Wq89tql8beX7VNdzxIEjnq/+CfrSzFBYA0yqWJ6Yryulzq5u1ryyg2c3buP5Tdt5btM21m/dxaZXd7HqpR2s3byD7hoflVtbxEEjhjBiyGsH7hFDWjl8TDvDh7QxvL2FEUPasvU9B/Z8m+HtbRUH+haGtbcyrC17PHxIdvBvb/WJZmZWXzNDYT5wmaRbgLcBmyNir66jgSQiWLt5J89t3MbKjdtYuWEbz258lec3bWfVy9vp6HrtqD+8vZXDxgxj7MghvHXyQUw8aALjDhjCuFFDGTtyCKOHtTNmeDujh7VzwLA2WlvqfOQ0M9sPkoWCpJuB04BxklYDXwLaASLiW8AC4GxgBbAd+ESqWl6P7u7g2U3beHzNZh5+4RUeX7OZJ1/cytadnb3bDGtvYfLBI5l2+CjeO+MwpowbyZRxIzny4BGMP2DoXs1TM7OBLlkoRMSFdZ4P4NJU7/96vLhlJ/c+tYF7ntrAA89sYtO23UD2qX/GhNHMOeEIjjl0FEeNP4Ap40dy6KhhtPgTvpkNIqUYaE5ld2c396/cxC9XbOQ/n9rAE+u2AjB+1FDefcx43jZlLMceMYZph42izX3yZvYG8IYLhYjgkVWv8P1fvcDCpevYkncHnTDpQP589jTeOXUcxx4x2l0/ZvaG9IYJhVUvbWf+o7/hRw+v4en1rzJySCuzZxzOOTMPY9bksYwe1t7sEs3Mmm5Qh0J3d/DLZzZx4y+f4+fLXwTgrZMP4q8+MIM5J0zggKGD+sc3M9tng/aoeN/TG/irnyzniXVbGTtyCH98+lQ+dPJEJo0d0ezSzMwGrEEXCtt2dXLVvy/jB4tXceTBI/j6h47nfTMPZ1h7a7NLMzMb8AZVKGze0cEF8x7giXVb+P13v5k/OeNoh4GZ2T4YNKHQ0dXNZd9/iKdf3MoNH38r75l2SLNLMjMrnUETCn/3sye47+mNXH3eTAeCmdnrNCiuyHrw+Zf59n89y4WnvIkPzZpU/wVmZtan0odCRPC/fvQ4h40exhfP+a1ml2NmVmqlD4Wn17/KsrVb+KPTjmKkrzswM/tvKX0o/HTJOiR477GHNbsUM7PSK38oPL6Wk990EIeMHtbsUszMSq/UofDcxm08sW4rs2e4lWBm1gilDoX/fGoDAGdOdyiYmTVCqUPhwedf5rDRw5g0dnizSzEzGxRKHQrL1m5hxoQxvveBmVmDlDYUduzu4pkNr3LsEaObXYqZ2aBR2lB4cctOIuBNngrbzKxhShsK67fuAuCQ0UObXImZ2eBR4lDYCcD4UQ4FM7NGKW0obOhpKYzyRWtmZo1S2lBYv3UX7a3iwOHtzS7FzGzQKG8obNnFuAOG0tLi01HNzBqltKGw4dVdHOLxBDOzhiptKKzfspPxHk8wM2uo0obCxld3+cwjM7MGK20obNnZyejhvqmOmVkjlTIUurqD3Z3djGh3KJiZNVIpQ2H77k4ARgxpbXIlZmaDS0lDoQuAEUMdCmZmjVTuUHBLwcysoUoaCln30XCPKZiZNVRJQyFrKYx095GZWUMlDQVJsyU9KWmFpCv6eP5Nku6W9LCkxySdXWS/7j4yM0sjWShIagWuAc4CpgMXSppetdkXgVsj4kTgAuCfi+x7h7uPzMySSNlSOAVYERErI2I3cAswp2qbAHrupzkG+E2RHW/b5ZaCmVkKKUNhArCqYnl1vq7Sl4GPSloNLAA+3deOJM2VtFjS4g0bNrCzMwuFYe0OBTOzRmr2QPOFwI0RMRE4G7hJ0l41RcS8iJgVEbPGjx9PZ1cA0N7qabPNzBopZSisASZVLE/M11X6JHArQETcDwwDxtXbcUdXNwBtLc3ONDOzwSXlUXURMFXSFElDyAaS51dt8wJwOoCk3yILhQ31dtzVnbUU2txSMDNrqGShEBGdwGXAQmA52VlGSyVdJencfLPPAZdIehS4Gbg4IqLevjsdCmZmSSQ9pzMiFpANIFeuu7Li8TLg1H3db0/3Ubu7j8zMGqqUR9XOrqBF+P7MZmYNVspQ6Ojupq21lKWbmQ1opTyydnYF7W4lmJk1XElDwS0FM7MUSnlk7egOX7hmZpZAKUOhs6vbF66ZmSVQyiNrZ1f4GgUzswRKGQpZ91EpSzczG9BKeWTt7Oqm1WcfmZk1XDlDoTtocyiYmTVcOUOhq9vdR2ZmCZTyyNrZ7YFmM7MUShkKHV3dngzPzCyBUh5ZfUqqmVkapQyFju7wNBdmZgmU8sja2dXtCfHMzBIoaSi4+8jMLIVShoLvp2BmlkYpj6y+n4KZWRolDYVuWn1KqplZw5XyyNrp+ymYmSVR2lDwQLOZWeOVMhQ6fJMdM7MkSnlk7exy95GZWQrlDAWfkmpmlkQpj6wdPiXVzCyJ0oVC5N/dUjAza7zyHVnzVPDZR2ZmjVe6UIg8FXw/BTOzxivdkTXcUjAzS6Z8oZB/95iCmVnjle7IGnlToVVuKZiZNVr5QiH/7oaCmVnj7dOhVdJBkmamKqaQPBU8S6qZWePVPbJKukfSaEljgYeA6yR9o8jOJc2W9KSkFZKu6Geb8yUtk7RU0vfr7dMtBTOzdNoKbDMmIrZI+hTw3Yj4kqTH6r1IUitwDfC7wGpgkaT5EbGsYpupwBeAUyPiZUmHFC28xWMKZmYNV+Tzdpukw4HzgR/vw75PAVZExMqI2A3cAsyp2uYS4JqIeBkgItbX22n0dh85FMzMGq1IKFwFLASeiYhFkt4MPF3gdROAVRXLq/N1lY4Gjpb0C0kPSJrd144kzZW0WNLil19+CfDZR2ZmKdTtPoqI24DbKpZXAh9s4PtPBU4DJgL3SjouIl6pqmEeMA/g2ONPjG1Ai1sKZmYNV2Sg+WhJd0p6PF+eKemLBfa9BphUsTwxX1dpNTA/Ijoi4lngKbKQ6F9P95FbCmZmDVek++g6ssHgDoCIeAy4oMDrFgFTJU2RNCR/zfyqbX5E1kpA0jiy7qSVtXb62tlHDgUzs0YrEgojIuLXVes6670oIjqBy8jGI5YDt0bEUklXSTo332whsEnSMuBu4PKI2FRnz1nhDgUzs4YrckrqRklHkR+NJZ0HrC2y84hYACyoWndlxeMA/jT/KiTcfWRmlkyRULiUbJB3mqQ1wLPARUmrKsAXNJuZNV7NUJDUAsyKiDMkjQRaImLr/imtb71jCm4pmJk1XM3P2xHRDfxZ/nhbswMBqJj7yKFgZtZoRTphfi7p85ImSRrb85W8sn70tBTkloKZWcMVGVP4cP790op1Aby58eUUkd9PwS0FM7OGK3JF85T9UUhRHlMwM0unbihIagf+EHhXvuoe4NqI6EhYV/+ip66mvLuZ2aBWpPvom0A78M/58sfydZ9KVVQtvqLZzCydIqHw1og4vmL5LkmPpiqoKIeCmVnjFTn7qCu/ohmAfOrsrnQl1dbTUnAmmJk1XpGWwuXA3ZJWAgKOBD6RtKpa8lTwndfMzBqvyNlHd+a3zTwmX/VkROxKW1bNigCHgplZCkXup3ApMDwiHsunzR4h6Y/Sl9Y3DzSbmaVTZEzhkso7oeX3U74kXUl1+JRUM7NkioRCqyrmlJDUCgxJV1JtbimYmaVTZKD5Z8APJF2bL/9+vq6pfEWzmVnjFQmFPwfmkl3VDHAH8O1kFdXhCfHMzNIpcvZRN/At4Fv57KgTI6Jp1yl46mwzs3SKnH10j6TReSA8CFwn6e/Tl9a36D0ltVkVmJkNXkUGmsdExBbgfwLfjYi3AaenLau+FqeCmVnDFQmFNkmHA+cDP05cT12vTXPhUDAza7QioXAVsBBYERGL8rmPnk5bVg09YwoOBTOzhisy0HwbcFvF8krggymLqlkP2QRMzgQzs8Yr0lIYkHz2kZlZ45UvFDxLqplZMv2GgqQxNZ6blaac+nxKqplZOrVaCj+XdFD1SklnAj9MV1J9kq9oNjNLoVYozCO7uc74nhWSPgJcC5yTurD+BD7zyMwslX7PPoqI6yTtJLsn85nAh4E/AN4TEc/tp/r6KMzjCWZmqdQ8JTUibsqD4WHgBeB3ImLjfqmsBmeCmVka/YaCpCW8dlnACOBgslaDgIiImfunxD0FPh3VzCyVWi2F9+23KvaRxxTMzNKoFQrtwKER8YvKlZJOBdYlraqGiHD3kZlZIrXOPvoHYEsf67fkzzWNZ0g1M0ujVigcGhFLqlfm6yYnq6gOn5JqZpZOrVA4sMZzw4vsXNJsSU9KWiHpihrbfVBSFL1S2heumZmlUSsUFku6pHqlpE+R3YGtJkmtwDXAWcB04EJJ0/vYbhTwGeBXhYt2JpiZJVFroPmzwA8lXcRrITALGAJ8oMC+TyG7B8NKAEm3AHOAZVXb/W/ga8DlRQoOX7xmZpZMvy2FiHgxIt4BfAV4Lv/6SkT8dkQUOftoArCqYnl1vq6XpJOASRHxk1o7kjRX0mJJi3fu3OnrFMzMEql18dowsmkt3gIsAa6PiM5GvbGkFuAbwMX1to2IeWRzMXHoUdPDDQUzszRqjSl8h6y7aAnZuMDX93Hfa4BJFcsT83U9RgEzgHskPQe8HZhfZLDZ3UdmZmnUGlOYHhHHAUi6Hvj1Pu57ETBV0hSyMLgA+EjPkxGxGRjXsyzpHuDzEbG45l7DA81mZqnUail09Dx4Pd1G+WsuAxYCy4FbI2KppKsknbvPlfbsF1+8ZmaWSq2WwvGSeq5oFjA8X+6ZEG90vZ1HxAJgQdW6K/vZ9rRCFePuIzOzVGrdT6F1fxZSVLj7yMwsmVrdRwOWWwpmZmk4FMzMrFfpQiEIWkpXtZlZOZTv8BqeJdXMLJXShULgWVLNzFIpXSiAzz4yM0ullKHgCfHMzNIoXShEuPvIzCyV0oUCuPvIzCyV0oVCEL5OwcwskdKFAnhMwcwsldKFgscUzMzSKV0ogMcUzMxSKWUo+IpmM7M0ShcKvqLZzCyd0oWCb8dpZpZO6UIhCJ99ZGaWSOlCAXw/BTOzVEoZCs4EM7M0ShcKEb54zcwsldKFArj7yMwsldKFQnZKarOrMDMbnEoXCuCL18zMUilfKIS7j8zMUildKARBS+mqNjMrh1IeXj3NhZlZGqULhcBjCmZmqZQuFMBzH5mZpVK6UPBNdszM0ildKICvaDYzS6WUoeBMMDNLo3ShEBG+TsHMLJHShQJAi5sKZmZJJA0FSbMlPSlphaQr+nj+TyUtk/SYpDslHVlkv84EM7M0koWCpFbgGuAsYDpwoaTpVZs9DMyKiJnA7cDf1dtv4GkuzMxSSdlSOAVYERErI2I3cAswp3KDiLg7Irbniw8AE4vs2KFgZpZGylCYAKyqWF6dr+vPJ4Gf9vWEpLmSFktaDA4FM7NUBsRAs6SPArOAq/t6PiLmRcSsiJgFHlMwM0ulLeG+1wCTKpYn5uv2IOkM4C+Bd0fEriI79tlHZmZppGwpLAKmSpoiaQhwATC/cgNJJwLXAudGxPqiO3b3kZlZGslCISI6gcuAhcBy4NaIWCrpKknn5ptdDRwA3CbpEUnz+9ndHtxQMDNLI2X3ERGxAFhQte7KisdnvJ79uqVgZpbGgBho3leeEM/MLI1ShkJbq0PBzCyFUoaCWwpmZmmUMxQ8pmBmlkQ5Q8EtBTOzJEoZCh5TMDNLo5Sh4FNSzczSKGUotLWUsmwzswGvlEdXjymYmaVRylBocyiYmSVRylBwS8HMLA2HgpmZ9SplKLj7yMwsjVKGgm+yY2aWRilDwS0FM7M0ShkKHlMwM0ujlKHgi9fMzNIo5dHVmWBmlkYpD69uKZiZpVHKo6vHFMzM0nAomJlZr1KGgk9JNTNLo5Sh4JaCmVkaDgUzM+vlUDAzs16lDAWPKZiZpVHKUHBLwcwsDYeCmZn1ciiYmVmvUoaCp7kwM0ujlEdXNxTMzNIoZShITgUzsxRKFwqOAzOzdEoXCi1uJZiZJVO6UBg5tK3ZJZiZDVpJQ0HSbElPSloh6Yo+nh8q6Qf587+SNLnePo88eESKUs3MjIShIKkVuAY4C5gOXChpetVmnwRejoi3AH8PfC1VPWZmVl/KlsIpwIqIWBkRu4FbgDlV28wBvpM/vh04XT61yMysaVJ20E8AVlUsrwbe1t82EdEpaTNwMLCxciNJc4G5+eIuSY8nqTiNcVT9PCVQtppdb1quN739UfORRTYqxahtRMwD5gFIWhwRs5pcUmFlqxfKV7PrTcv1pjeQak7ZfbQGmFSxPDFf1+c2ktqAMcCmhDWZmVkNKUNhETBV0hRJQ4ALgPlV28wHPp4/Pg+4KyIiYU1mZlZDsu6jfIzgMmAh0ArcEBFLJV0FLI6I+cD1wE2SVgAvkQVHPfNS1ZxI2eqF8tXsetNyvekNmJrlD+ZmZtajdFc0m5lZOg4FMzPrVapQqDdtxkAiaZKkuyUtk7RU0meaXVMRklolPSzpx82upR5JB0q6XdITkpZL+u1m11SLpD/J/xYel3SzpGHNrqmapBskra+8FkjSWEl3SHo6/35QM2us1E+9V+d/E49J+qGkA5tZY6W+6q147nOSQtK4ZtTWozShUHDajIGkE/hcREwH3g5cOsDr7fEZYHmziyjoH4GfRcQ04HgGcN2SJgB/DMyKiBlkJ18UObFif7sRmF217grgzoiYCtyZLw8UN7J3vXcAMyJiJvAU8IX9XVQNN7J3vUiaBJwJvLC/C6pWmlCg2LQZA0ZErI2Ih/LHW8kOWBOaW1VtkiYC5wDfbnYt9UgaA7yL7Aw2ImJ3RLzS3KrqagOG59fkjAB+0+R69hIR95KdCVipcjqa7wDv369F1dBXvRHxHxHRmS8+QHaN1IDQz+8Xsrnf/gxo+pk/ZQqFvqbNGNAH2R757K8nAr9qbiV1/QPZH2Z3swspYAqwAfi/eXfXtyWNbHZR/YmINcDXyT4JrgU2R8R/NLeqwg6NiLX543XAoc0sZh/9HvDTZhdRi6Q5wJqIeLTZtUC5QqGUJB0A/D/gsxGxpdn19EfS+4D1EfFgs2spqA04CfhmRJwIbGNgdWvsIe+Hn0MWZkcAIyV9tLlV7bv84tKmf5otQtJfknXjfq/ZtfRH0gjgL4Arm11LjzKFQpFpMwYUSe1kgfC9iPjXZtdTx6nAuZKeI+ua+x+S/qW5JdW0GlgdET2tr9vJQmKgOgN4NiI2REQH8K/AO5pcU1EvSjocIP++vsn11CXpYuB9wEUDfJaEo8g+KDya/+9NBB6SdFizCipTKBSZNmPAyKcAvx5YHhHfaHY99UTEFyJiYkRMJvvd3hURA/aTbESsA1ZJOiZfdTqwrIkl1fMC8HZJI/K/jdMZwAPjVSqno/k48G9NrKUuSbPJukHPjYjtza6nlohYEhGHRMTk/H9vNXBS/vfdFKUJhXzgqGfajOXArRGxtLlV1XQq8DGyT9yP5F9nN7uoQebTwPckPQacAPx1k+vpV96iuR14CFhC9r83YKY26CHpZuB+4BhJqyV9Evhb4HclPU3W4vnbZtZYqZ96/wkYBdyR/999q6lFVuin3gHF01yYmVmv0rQUzMwsPYeCmZn1ciiYmVkvh4KZmfVyKJiZWS+HgpWWpK78lMPHJd2WXx2KpMMk3SLpGUkPSlog6WhJk6tm07wkf/6gqv2+v3LyQkn3SGr4TdUlPbcvM2JKuljSP/Xz3KuNq8zeyBwKVmY7IuKEfNbR3cAf5BeG/RC4JyKOioiTyWbJ3GO+HkkfI7vO4b0R8XLVft9PNhNvYfkkd2al51CwweI+4C3Ae4COiOi9YCkiHo2I+3qWJZ1PNk/SmRGxsXInkt4BnAtcnbdCjsqf+pCkX0t6StI7820vljRf0l1kU0oj6XJJi/K5/L+Srxsp6SeSHs1bNR+ueMtPS3pI0hJJ0/Ltx0r6Ub6PByTNrP5h8yv7789f99WK9YdLureiBfXO1/8rtTcih4KVXv4p/SyyK4VnALUm9TuS7IrXM/uaSiAifkk2rcPleSvkmfyptog4Bfgs8KWKl5wEnBcR75Z0JjCVbJr3E4CTJb2LbP7830TE8Xmr5mcVr98YEScB3wQ+n6/7CvBwfj+AvwC+28fP8Y9kkwEeRzbrao+PAAsj4gSye0w8UuN3YbYXh4KV2XBJjwCLyeYWur7Aazbk256/j+/VM6Hhg8DkivV3RETP/Phn5l8Pk01nMY0sJJaQTRPxNUnvjIjNdfb7O8BNABFxF3CwpNFV9ZwK3Jw/vqli/SLgE5K+DByX38vDrDD3g1qZ7cg/EfeStBQ4r8ZrtgNnA/dJWh8RRadV3pV/72LP/5ttlW8P/E1EXFv9Ykkn5e/7VUl3RsRVdfZbxF5z1ETEvXnr5BzgRknfiIi+WhpmfXJLwQabu4Chkub2rJA0s7JvPSLWk3Xp/LWk9/axj61kE6rtq4XA7+X30EDSBEmHSDoC2B4R/wJcTf0pvu8DLsr3cRpZF1P1vTh+wWu387yoZ6WkI4EXI+I6sjvoDeTpxG0AcijYoJLPnf8B4Iz8lNSlwN+Q3TGscrtnyQaUb5B0StVubgEuV3ZHt6MoKL+T2veB+yUtIZsVdRRwHPDrvKvrS8BX+98LAF8mG494jGxG0o/3sc1nyO77vYQ970B4Gtnc/A8DHyYbezArzLOkmplZL7cUzMysl0PBzMx6ORTMzKyXQ8HMzHo5FMzMrJdDwczMejkUzMys1/8HaH69416GUfgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. PCK 0.9052319309600863\n",
      "----------------------------------\n",
      "Prediction time: 293.83032298088074. Average 0.32007660455433634/image\n",
      "Total time:  389.1849241256714\n"
     ]
    }
   ],
   "source": [
    " evaluate_jump(model, dataset_test, jump_dataset, EVAL_TYPE, limit=LIMIT, training_heads=HEADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1-12-gpu",
   "language": "python",
   "name": "tf1-12-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
