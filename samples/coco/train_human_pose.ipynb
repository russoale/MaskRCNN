{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Human pose estimation\n",
    "\n",
    "Inspect and visualize data loading and pre-processing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "# Import COCO config\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n",
    "import coco\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory to hdd \n",
    "DATA_DIR = os.path.abspath(\"/data/hdd/\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(DATA_DIR, \"russales\", \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(DATA_DIR, \"russales\", \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Skeleton: (19, 2)\n",
      "Keypoint names: (17,)\n",
      "loading annotations into memory...\n",
      "Done (t=0.26s)\n",
      "creating index...\n",
      "index created!\n",
      "Skeleton: (19, 2)\n",
      "Keypoint names: (17,)\n",
      "Train Keypoints Image Count: 64115\n",
      "Train Keypoints Class Count: 2\n",
      "  0. BG                                                \n",
      "  1. person                                            \n",
      "Val Keypoints Image Count: 2693\n",
      "Val Keypoints Class Count: 2\n",
      "  0. BG                                                \n",
      "  1. person                                            \n"
     ]
    }
   ],
   "source": [
    "class TrainConfig(coco.CocoConfig):\n",
    "    GPU_COUNT=1\n",
    "\n",
    "# MS COCO Dataset\n",
    "config = TrainConfig()\n",
    "COCO_DIR = os.path.join(DATA_DIR, \"Datasets\", \"MSCOCO_2017\")  # TODO: enter value here\n",
    "# Load dataset\n",
    "assert config.NAME == \"coco\"\n",
    "# Training dataset\n",
    "# load person keypoints dataset\n",
    "train_dataset_keypoints = coco.CocoDataset(task_type=\"person_keypoints\")\n",
    "train_dataset_keypoints.load_coco(COCO_DIR, \"train\", year=\"2017\")\n",
    "train_dataset_keypoints.prepare()\n",
    "\n",
    "#Validation dataset\n",
    "val_dataset_keypoints = coco.CocoDataset(task_type=\"person_keypoints\")\n",
    "val_dataset_keypoints.load_coco(COCO_DIR, \"val\", year=\"2017\")\n",
    "val_dataset_keypoints.prepare()\n",
    "\n",
    "print(\"Train Keypoints Image Count: {}\".format(len(train_dataset_keypoints.image_ids)))\n",
    "print(\"Train Keypoints Class Count: {}\".format(train_dataset_keypoints.num_classes))\n",
    "for i, info in enumerate(train_dataset_keypoints.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))\n",
    "\n",
    "print(\"Val Keypoints Image Count: {}\".format(len(val_dataset_keypoints.image_ids)))\n",
    "print(\"Val Keypoints Class Count: {}\".format(val_dataset_keypoints.num_classes))\n",
    "for i, info in enumerate(val_dataset_keypoints.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  /data/hdd/russales/mask_rcnn_coco.h5\n"
     ]
    }
   ],
   "source": [
    "# Create model object in training mode.\n",
    "model = modellib.MaskRCNN(mode=\"training\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True,exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "print(\"Loading weights from \", COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train heads\n",
      "\n",
      "Starting at epoch 0. LR=0.002\n",
      "\n",
      "Checkpoint Path: /data/hdd/russales/logs/coco20181201T1651/mask_rcnn_coco_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_keypoint_mask_conv1   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn1   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv2   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn2   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv3   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn3   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv4   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn4   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv5   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn5   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv6   (TimeDistributed)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn6   (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv7   (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn7   (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv8   (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn8   (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_keypoint_mask_deconv   (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/russales/workspace/MaskRCNN/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/russales/workspace/MaskRCNN/venv/lib/python3.5/site-packages/keras/engine/training.py:2033: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 921s 921ms/step - loss: 8.3424 - rpn_class_loss: 0.0306 - rpn_bbox_loss: 0.4068 - mrcnn_class_loss: 0.2311 - mrcnn_bbox_loss: 0.4108 - keypoint_mrcnn_mask_loss: 6.8874 - mrcnn_mask_loss: 0.3757 - val_loss: 7.9140 - val_rpn_class_loss: 0.0067 - val_rpn_bbox_loss: 0.3196 - val_mrcnn_class_loss: 0.1561 - val_mrcnn_bbox_loss: 0.3561 - val_keypoint_mrcnn_mask_loss: 6.6990 - val_mrcnn_mask_loss: 0.3764\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 853s 853ms/step - loss: 7.5221 - rpn_class_loss: 0.0258 - rpn_bbox_loss: 0.3904 - mrcnn_class_loss: 0.2120 - mrcnn_bbox_loss: 0.3356 - keypoint_mrcnn_mask_loss: 6.2224 - mrcnn_mask_loss: 0.3358 - val_loss: 7.0936 - val_rpn_class_loss: 0.0074 - val_rpn_bbox_loss: 0.2295 - val_mrcnn_class_loss: 0.1476 - val_mrcnn_bbox_loss: 0.3537 - val_keypoint_mrcnn_mask_loss: 6.0625 - val_mrcnn_mask_loss: 0.2930\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Training - Stage 1\n",
    "print(\"Train heads\")\n",
    "model.train(train_dataset_keypoints, val_dataset_keypoints,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=2,\n",
    "            layers='heads')\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Resnet layer 4+\n",
      "\n",
      "Starting at epoch 2. LR=0.0002\n",
      "\n",
      "Checkpoint Path: /data/hdd/russales/logs/coco20181201T1651/mask_rcnn_coco_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_keypoint_mask_conv1   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn1   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv2   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn2   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv3   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn3   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv4   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn4   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv5   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn5   (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv6   (TimeDistributed)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn6   (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv7   (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn7   (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_keypoint_mask_conv8   (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_keypoint_mask_bn8   (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_keypoint_mask_deconv   (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/russales/workspace/MaskRCNN/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/russales/workspace/MaskRCNN/venv/lib/python3.5/site-packages/keras/engine/training.py:2033: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8\n",
      "1000/1000 [==============================] - 971s 971ms/step - loss: 7.2145 - rpn_class_loss: 0.0236 - rpn_bbox_loss: 0.3859 - mrcnn_class_loss: 0.1987 - mrcnn_bbox_loss: 0.3030 - keypoint_mrcnn_mask_loss: 5.9822 - mrcnn_mask_loss: 0.3211 - val_loss: 7.9834 - val_rpn_class_loss: 0.0186 - val_rpn_bbox_loss: 0.5841 - val_mrcnn_class_loss: 0.2411 - val_mrcnn_bbox_loss: 0.4553 - val_keypoint_mrcnn_mask_loss: 6.3033 - val_mrcnn_mask_loss: 0.3811\n",
      "Epoch 4/8\n",
      "1000/1000 [==============================] - 941s 941ms/step - loss: 7.0967 - rpn_class_loss: 0.0222 - rpn_bbox_loss: 0.3586 - mrcnn_class_loss: 0.1887 - mrcnn_bbox_loss: 0.2901 - keypoint_mrcnn_mask_loss: 5.9211 - mrcnn_mask_loss: 0.3160 - val_loss: 7.7850 - val_rpn_class_loss: 0.0156 - val_rpn_bbox_loss: 0.5200 - val_mrcnn_class_loss: 0.2180 - val_mrcnn_bbox_loss: 0.4676 - val_keypoint_mrcnn_mask_loss: 6.1773 - val_mrcnn_mask_loss: 0.3865\n",
      "Epoch 5/8\n",
      "1000/1000 [==============================] - 940s 940ms/step - loss: 7.0055 - rpn_class_loss: 0.0205 - rpn_bbox_loss: 0.3545 - mrcnn_class_loss: 0.1814 - mrcnn_bbox_loss: 0.2833 - keypoint_mrcnn_mask_loss: 5.8500 - mrcnn_mask_loss: 0.3157 - val_loss: 7.8083 - val_rpn_class_loss: 0.0150 - val_rpn_bbox_loss: 0.4350 - val_mrcnn_class_loss: 0.1939 - val_mrcnn_bbox_loss: 0.4483 - val_keypoint_mrcnn_mask_loss: 6.3304 - val_mrcnn_mask_loss: 0.3858\n",
      "Epoch 6/8\n",
      "1000/1000 [==============================] - 942s 942ms/step - loss: 6.9099 - rpn_class_loss: 0.0202 - rpn_bbox_loss: 0.3419 - mrcnn_class_loss: 0.1784 - mrcnn_bbox_loss: 0.2758 - keypoint_mrcnn_mask_loss: 5.7888 - mrcnn_mask_loss: 0.3049 - val_loss: 7.7332 - val_rpn_class_loss: 0.0117 - val_rpn_bbox_loss: 0.4307 - val_mrcnn_class_loss: 0.2225 - val_mrcnn_bbox_loss: 0.4427 - val_keypoint_mrcnn_mask_loss: 6.2510 - val_mrcnn_mask_loss: 0.3745\n",
      "Epoch 7/8\n",
      "1000/1000 [==============================] - 938s 938ms/step - loss: 6.8484 - rpn_class_loss: 0.0181 - rpn_bbox_loss: 0.3343 - mrcnn_class_loss: 0.1750 - mrcnn_bbox_loss: 0.2673 - keypoint_mrcnn_mask_loss: 5.7516 - mrcnn_mask_loss: 0.3021 - val_loss: 7.5610 - val_rpn_class_loss: 0.0160 - val_rpn_bbox_loss: 0.5312 - val_mrcnn_class_loss: 0.1856 - val_mrcnn_bbox_loss: 0.4238 - val_keypoint_mrcnn_mask_loss: 6.0194 - val_mrcnn_mask_loss: 0.3850\n",
      "Epoch 8/8\n",
      " 933/1000 [==========================>...] - ETA: 1:02 - loss: 6.8689 - rpn_class_loss: 0.0198 - rpn_bbox_loss: 0.3371 - mrcnn_class_loss: 0.1722 - mrcnn_bbox_loss: 0.2643 - keypoint_mrcnn_mask_loss: 5.7774 - mrcnn_mask_loss: 0.2982"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'path': '/data/hdd/Datasets/MSCOCO_2017/train2017/000000202582.jpg', 'id': 202582, 'source': 'coco', 'annotations': [{'image_id': 202582, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 199488, 'num_keypoints': 0, 'bbox': [589.65, 204.19, 27.03, 33.31], 'area': 280.60025, 'category_id': 1, 'segmentation': [[597.61, 234.84, 598.09, 225.91, 605.33, 225.67, 611.61, 225.91, 615.71, 220.6, 616.68, 212.64, 613.78, 207.33, 610.64, 204.19, 607.99, 205.88, 605.82, 210.95, 609.68, 212.16, 607.75, 216.74, 605.58, 221.33, 595.92, 217.47, 593.27, 231.47, 590.37, 236.05, 589.65, 237.5, 593.99, 236.05, 597.61, 233.64]], 'iscrowd': 0}, {'image_id': 202582, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 263255, 'num_keypoints': 0, 'bbox': [526.72, 209.51, 8.81, 8.4], 'area': 49.1088, 'category_id': 1, 'segmentation': [[527.95, 217.91, 526.72, 214.63, 528.16, 211.56, 531.02, 209.72, 532.25, 209.51, 533.69, 210.13, 534.1, 211.15, 534.3, 213.2, 534.71, 213.81, 535.53, 215.45, 535.53, 216.27, 535.53, 216.27, 535.53, 216.89, 527.75, 217.09]], 'iscrowd': 0}, {'image_id': 202582, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 565383, 'num_keypoints': 0, 'bbox': [515.44, 200.3, 10.39, 24.18], 'area': 103.9508, 'category_id': 1, 'segmentation': [[521.11, 224.48, 519.41, 222.22, 517.52, 215.98, 517.52, 211.83, 515.44, 208.8, 518.27, 205.97, 517.33, 202.19, 521.3, 200.3, 521.48, 205.03, 524.88, 207.48, 525.83, 208.05, 523.94, 212.2, 521.86, 214.47, 520.92, 218.63, 520.92, 223.92]], 'iscrowd': 0}, {'image_id': 202582, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 565778, 'num_keypoints': 0, 'bbox': [548.13, 210.06, 16.66, 19.29], 'area': 137.87435, 'category_id': 1, 'segmentation': [[564.69, 219.5, 560.49, 225.26, 558.4, 225.26, 556.62, 225.68, 554.31, 224.53, 553.58, 223.48, 550.64, 229.04, 550.22, 229.35, 548.96, 228.83, 548.13, 227.04, 549.38, 225.79, 549.7, 225.26, 550.12, 224.32, 550.33, 223.37, 550.64, 221.07, 550.75, 220.23, 551.06, 219.29, 551.16, 218.45, 551.48, 217.82, 552.63, 216.67, 554.2, 217.19, 555.57, 217.71, 556.62, 218.24, 557.24, 218.24, 559.03, 218.03, 559.34, 217.71, 560.39, 216.88, 559.55, 213.94, 559.34, 213.42, 559.03, 212.47, 559.97, 210.59, 562.38, 210.06, 563.74, 211.43, 564.06, 212.89, 564.69, 214.67, 564.79, 217.19, 564.79, 217.3]], 'iscrowd': 0}, {'image_id': 202582, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 569147, 'num_keypoints': 0, 'bbox': [545.56, 213.05, 3.21, 10.3], 'area': 21.7178, 'category_id': 1, 'segmentation': [[546.54, 222.56, 548.74, 217.28, 548.77, 215.51, 548.34, 213.67, 545.56, 213.05, 545.62, 215.25, 545.72, 218.86, 545.95, 221.28, 545.79, 223.35]], 'iscrowd': 0}, {'image_id': 202582, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 1219258, 'num_keypoints': 0, 'bbox': [500.55, 209.57, 5.02, 7.99], 'area': 26.6157, 'category_id': 1, 'segmentation': [[500.66, 216.31, 501.34, 212.88, 500.77, 210.94, 502.26, 209.57, 504.66, 209.57, 505, 212.31, 505.57, 216.54, 504.31, 216.54, 503.86, 214.02, 502.83, 214.59, 502.37, 217.56, 500.55, 217.56]], 'iscrowd': 0}, {'image_id': 202582, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 1722287, 'num_keypoints': 0, 'bbox': [638.34, 207.71, 1.66, 7.35], 'area': 7.35835, 'category_id': 1, 'segmentation': [[640, 207.71, 639.01, 207.89, 638.34, 209.91, 638.58, 211.14, 639.26, 212.61, 639.87, 213.16, 639.13, 215.06, 640, 214.88]], 'iscrowd': 0}, {'image_id': 202582, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 1753707, 'num_keypoints': 0, 'bbox': [593.65, 200.77, 6.52, 8.4], 'area': 28.06375, 'category_id': 1, 'segmentation': [[595.15, 208.92, 594.78, 207.04, 594.02, 205.28, 593.65, 203.03, 593.77, 202.4, 594.4, 201.52, 595.15, 200.77, 596.28, 201.02, 596.41, 202.27, 596.28, 202.65, 596.91, 203.65, 597.54, 205.03, 598.41, 205.53, 599.54, 206.04, 600.17, 206.54, 599.79, 207.54, 598.79, 207.92, 598.04, 207.92, 597.41, 208.67, 597.54, 209.17]], 'iscrowd': 0}, {'image_id': 202582, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 2000452, 'num_keypoints': 0, 'bbox': [574.71, 201.28, 4.28, 4.28], 'area': 11.1194, 'category_id': 1, 'segmentation': [[575.04, 205.56, 574.71, 202.92, 577.02, 201.28, 578.99, 204.9]], 'iscrowd': 0}, {'image_id': 202582, 'keypoints': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': 2030227, 'num_keypoints': 0, 'bbox': [592.16, 207.8, 3.28, 1.93], 'area': 3.5, 'category_id': 1, 'segmentation': [[592.16, 209.49, 592.24, 209.09, 592.64, 208.76, 592.72, 208.52, 592.8, 208.04, 593.28, 207.8, 593.84, 207.96, 594.08, 208.6, 594.24, 208.68, 595.04, 208.92, 595.44, 209.73]], 'iscrowd': 0}], 'width': 640, 'height': 480}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/russales/workspace/MaskRCNN/mrcnn/data_generator.py\", line 604, in data_generator_keypoint\n",
      "    load_image_gt_keypoints(dataset, config, image_id, augment, use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/russales/workspace/MaskRCNN/mrcnn/data_generator.py\", line 527, in load_image_gt_keypoints\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/russales/workspace/MaskRCNN/mrcnn/utils.py\", line 606, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 938s 938ms/step - loss: 6.8649 - rpn_class_loss: 0.0196 - rpn_bbox_loss: 0.3352 - mrcnn_class_loss: 0.1720 - mrcnn_bbox_loss: 0.2647 - keypoint_mrcnn_mask_loss: 5.7738 - mrcnn_mask_loss: 0.2996 - val_loss: 7.3764 - val_rpn_class_loss: 0.0138 - val_rpn_bbox_loss: 0.4949 - val_mrcnn_class_loss: 0.2258 - val_mrcnn_bbox_loss: 0.3748 - val_keypoint_mrcnn_mask_loss: 5.8772 - val_mrcnn_mask_loss: 0.3900\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Resnet layer 4+\")\n",
    "model.train(train_dataset_keypoints, val_dataset_keypoints,\n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=8,\n",
    "            layers='4+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training All Layers\")\n",
    "model.train(train_dataset_keypoints, val_dataset_keypoints,\n",
    "            learning_rate=config.LEARNING_RATE / 100,\n",
    "            epochs=20,\n",
    "            layers='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train human pose MSCOCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mask_rcnn",
   "language": "python",
   "name": "mask_rcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
